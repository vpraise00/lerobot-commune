# Heuristic Policy - ì™„ë²½ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ì„¤ì¹˜ í™•ì¸](#ì„¤ì¹˜-í™•ì¸)
2. [ê¸°ë³¸ ì‚¬ìš©ë²•](#ê¸°ë³¸-ì‚¬ìš©ë²•)
3. [ì›ê²© ì œì–´ (PolicyServer)](#ì›ê²©-ì œì–´)
4. [ì»¤ìŠ¤í…€ íœ´ë¦¬ìŠ¤í‹± ë§Œë“¤ê¸°](#ì»¤ìŠ¤í…€-íœ´ë¦¬ìŠ¤í‹±)
5. [ë°ì´í„° ìˆ˜ì§‘](#ë°ì´í„°-ìˆ˜ì§‘)
6. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ì„¤ì¹˜ í™•ì¸

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ
cd /path/to/lerobot

# ì„¤ì¹˜ í™•ì¸
python -c "from lerobot.policies.heuristic import HeuristicPolicy; print('âœ… ì„¤ì¹˜ ì™„ë£Œ')"

# Vision ê¸°ëŠ¥ ì‚¬ìš©ì‹œ OpenCV ì„¤ì¹˜
pip install opencv-python
```

---

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ë¡œì»¬ì—ì„œ ì§ì ‘ ì œì–´

```python
from lerobot.policies.heuristic import HeuristicPolicy, HeuristicConfig
from lerobot.robots.so101_follower import SO101Follower, SO101FollowerConfig
import torch

# ========== ì„¤ì • ==========
robot_config = SO101FollowerConfig(
    port="/dev/ttyACM0",  # lerobot-find-portë¡œ ì°¾ì€ í¬íŠ¸
    id="my_so101"
)

policy_config = HeuristicConfig(
    gripper_threshold=0.5,   # ê·¸ë¦¬í¼ ì—´ê¸°/ë‹«ê¸° ì„ê³„ê°’
    speed_factor=1.0,        # ë™ì‘ ì†ë„ ë°°ìœ¨
    use_vision=False         # ë¹„ì „ ì‚¬ìš© ì—¬ë¶€
)

# ========== ì´ˆê¸°í™” ==========
robot = SO101Follower(robot_config)
robot.connect()

policy = HeuristicPolicy(policy_config)

# ========== ì œì–´ ë£¨í”„ ==========
try:
    for step in range(100):
        # 1. ê´€ì°° ë°›ê¸°
        obs = robot.get_observation()

        # 2. Batch í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        batch = {
            "observation.state": torch.tensor([[
                obs["shoulder_pan.pos"],
                obs["shoulder_lift.pos"],
                obs["elbow_flex.pos"],
                obs["wrist_flex.pos"],
                obs["wrist_roll.pos"],
                obs["gripper.pos"],
            ]], dtype=torch.float32)
        }

        # 3. ì •ì±…ì—ì„œ ì•¡ì…˜ ë°›ê¸°
        action = policy.select_action(batch)

        # 4. ë¡œë´‡ì— ì•¡ì…˜ ì „ì†¡
        action_dict = {
            "shoulder_pan.pos": float(action[0, 0]),
            "shoulder_lift.pos": float(action[0, 1]),
            "elbow_flex.pos": float(action[0, 2]),
            "wrist_flex.pos": float(action[0, 3]),
            "wrist_roll.pos": float(action[0, 4]),
            "gripper.pos": float(action[0, 5]),
        }
        robot.send_action(action_dict)

        if step % 10 == 0:
            print(f"Step {step}: Gripper={action_dict['gripper.pos']:.2f}")

finally:
    robot.disconnect()
```

---

## ì›ê²© ì œì–´

### ì•„í‚¤í…ì²˜
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë¡œë´‡ ë¨¸ì‹         â”‚ gRPC observations  â”‚  ì„œë²„ ë¨¸ì‹         â”‚
â”‚  - SO101 ì—°ê²°    â”‚ -----------------> â”‚  - HeuristicPolicyâ”‚
â”‚  - RobotClient   â”‚ <----------------- â”‚  - PolicyServer   â”‚
â”‚                  â”‚    gRPC actions    â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1ë‹¨ê³„: ì„œë²„ ì‹œì‘ (ì„œë²„ ë¨¸ì‹ )

```bash
python src/lerobot/async_inference/policy_server.py \
    --host=0.0.0.0 \
    --port=8080 \
    --fps=30
```

### 2ë‹¨ê³„: í´ë¼ì´ì–¸íŠ¸ ì‹œì‘ (ë¡œë´‡ ë¨¸ì‹ )

```bash
python src/lerobot/async_inference/robot_client.py \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.cameras='{front: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}}' \
    --robot.id=my_so101 \
    --server_address=192.168.1.100:8080 \
    --policy_type=heuristic \
    --pretrained_name_or_path=dummy \
    --policy_device=cpu
```

**ì£¼ìš” íŒŒë¼ë¯¸í„°:**
- `--server_address`: ì„œë²„ IP:í¬íŠ¸
- `--policy_type=heuristic`: íœ´ë¦¬ìŠ¤í‹± ì •ì±… ì‚¬ìš©
- `--pretrained_name_or_path=dummy`: íœ´ë¦¬ìŠ¤í‹±ì€ ê°€ì¤‘ì¹˜ê°€ ì—†ìœ¼ë¯€ë¡œ dummy ê°’

---

## ì»¤ìŠ¤í…€ íœ´ë¦¬ìŠ¤í‹±

### ì˜ˆì œ 1: ê°„ë‹¨í•œ Pick-and-Place

```python
from lerobot.policies.heuristic.modeling_heuristic import HeuristicPolicy
from lerobot.policies.heuristic import HeuristicConfig
from torch import Tensor

class PickAndPlacePolicy(HeuristicPolicy):
    """4ë‹¨ê³„ Pick-and-Place íœ´ë¦¬ìŠ¤í‹±"""

    def _state_based_control(self, state: Tensor, action: Tensor) -> Tensor:
        """
        ìƒíƒœ ê¸°ë°˜ ì œì–´ ë¡œì§

        Args:
            state: í˜„ì¬ ìƒíƒœ (batch, 6) - [pan, lift, elbow, wrist, roll, gripper]
            action: ìˆ˜ì •í•  ì•¡ì…˜ (batch, 6)
        """
        step = self._step_count

        # Phase 1: ì ‘ê·¼ (0-50 steps)
        if step < 50:
            action[:, -1] = 1.0              # ê·¸ë¦¬í¼ ì—´ê¸°
            action[:, 1] = state[:, 1] - 0.01  # ì•„ë˜ë¡œ ì´ë™

        # Phase 2: ì¡ê¸° (50-100 steps)
        elif step < 100:
            action[:, -1] = 0.0              # ê·¸ë¦¬í¼ ë‹«ê¸°

        # Phase 3: ë“¤ì–´ì˜¬ë¦¬ê¸° (100-150 steps)
        elif step < 150:
            action[:, 1] = state[:, 1] + 0.02  # ìœ„ë¡œ ì´ë™

        # Phase 4: ë†“ê¸° (150+ steps)
        else:
            action[:, -1] = 1.0              # ê·¸ë¦¬í¼ ì—´ê¸°
            action[:, 0] = 0.5               # Panì„ íŠ¹ì • ìœ„ì¹˜ë¡œ

        return action


# ì‚¬ìš©
config = HeuristicConfig()
policy = PickAndPlacePolicy(config)
```

### ì˜ˆì œ 2: Vision ê¸°ë°˜ ì œì–´

```python
class VisionTrackingPolicy(HeuristicPolicy):
    """ë¹¨ê°„ ë¬¼ì²´ë¥¼ ë”°ë¼ê°€ëŠ” ì •ì±…"""

    def __init__(self, config):
        super().__init__(config)
        # ë¹„ì „ í™œì„±í™” í•„ìˆ˜
        config.use_vision = True

    def _vision_based_control(self, batch: dict, action: Tensor) -> Tensor:
        """ì´ë¯¸ì§€ì—ì„œ ë¬¼ì²´ë¥¼ ê°ì§€í•˜ê³  ë”°ë¼ê°"""

        # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        image_keys = [k for k in batch.keys() if k.startswith("observation.image")]
        if not image_keys:
            return action

        image = batch[image_keys[0]][0]  # (C, H, W)
        state = batch["observation.state"][0]

        # ë¬¼ì²´ ê°ì§€
        cx, cy = self._detect_object(image)

        if cx is not None:
            # ì´ë¯¸ì§€ ì¤‘ì‹¬ê³¼ì˜ ì˜¤ì°¨ ê³„ì‚°
            img_h, img_w = image.shape[1], image.shape[2]
            error_x = (cx - img_w/2) / (img_w/2)
            error_y = (cy - img_h/2) / (img_h/2)

            # ë¹„ë¡€ ì œì–´
            action[0, 0] = state[0] - error_x * 0.1  # Pan
            action[0, 1] = state[1] + error_y * 0.1  # Lift

            # ë¬¼ì²´ê°€ ì¤‘ì•™ì— ê°€ê¹Œìš°ë©´ ê·¸ë¦¬í¼ ë‹«ê¸°
            if abs(error_x) < 0.1 and abs(error_y) < 0.1:
                action[0, -1] = 0.0  # Close gripper

        return action


# ì‚¬ìš©
config = HeuristicConfig(
    use_vision=True,
    target_color_lower=[0, 100, 100],   # ë¹¨ê°„ìƒ‰ (HSV)
    target_color_upper=[10, 255, 255]
)
policy = VisionTrackingPolicy(config)
```

### ì˜ˆì œ 3: ì„í”¼ë˜ìŠ¤ ì œì–´

```python
class ImpedanceControlPolicy(HeuristicPolicy):
    """ë¶€ë“œëŸ¬ìš´ ì„í”¼ë˜ìŠ¤ ì œì–´"""

    def __init__(self, config):
        super().__init__(config)
        self.target_position = None
        self.stiffness = 0.5
        self.damping = 0.1

    def _state_based_control(self, state: Tensor, action: Tensor) -> Tensor:
        """ì„í”¼ë˜ìŠ¤ ì œì–´ ë¡œì§"""

        if self.target_position is None:
            # ëª©í‘œ ìœ„ì¹˜ ì„¤ì • (ì´ˆê¸° ìƒíƒœ ê¸°ì¤€)
            self.target_position = state.clone()
            self.target_position[:, 1] += 0.2  # ìœ„ìª½ìœ¼ë¡œ 20cm

        # ì„í”¼ë˜ìŠ¤ ì œì–´: F = K*(x_target - x) - D*v
        # ê°„ë‹¨í™”: velocityë¥¼ 0ìœ¼ë¡œ ê°€ì •
        error = self.target_position - state
        action = state + error * self.stiffness

        return action
```

---

## ë°ì´í„° ìˆ˜ì§‘

### íœ´ë¦¬ìŠ¤í‹± ì •ì±…ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°

```bash
lerobot-record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.cameras="{front: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}}" \
    --robot.id=my_so101 \
    --policy.path=dummy \
    --policy.type=heuristic \
    --dataset.repo_id=${HF_USER}/heuristic_dataset \
    --dataset.num_episodes=10 \
    --dataset.single_task="Pick and place cube"
```

**ì°¸ê³ :** `--policy.path=dummy`ëŠ” íœ´ë¦¬ìŠ¤í‹± ì •ì±…ì—ëŠ” ê°€ì¤‘ì¹˜ê°€ ì—†ì–´ì„œ dummy ê°’ ì‚¬ìš©

---

## ë¬¸ì œ í•´ê²°

### 1. ModuleNotFoundError: No module named 'lerobot.policies.heuristic'

```bash
# í™•ì¸
ls src/lerobot/policies/heuristic/

# ìˆì–´ì•¼ í•  íŒŒì¼ë“¤:
# - __init__.py
# - configuration_heuristic.py
# - modeling_heuristic.py
# - README.md

# ì—†ìœ¼ë©´ ë‹¤ì‹œ ìƒì„±
```

### 2. "heuristic policy not found" ì—ëŸ¬

**í™•ì¸ ì‚¬í•­:**
1. `src/lerobot/policies/__init__.py`ì— HeuristicConfig ì¶”ê°€ë¨
2. `src/lerobot/policies/factory.py`ì— heuristic ì¼€ì´ìŠ¤ ì¶”ê°€ë¨
3. `src/lerobot/async_inference/constants.py`ì— "heuristic" ì¶”ê°€ë¨

### 3. Vision ê¸°ëŠ¥ì´ ì•ˆ ë¨

```bash
# OpenCV ì„¤ì¹˜ í™•ì¸
python -c "import cv2; print(cv2.__version__)"

# ì„¤ì¹˜
pip install opencv-python
```

### 4. ë¡œë´‡ ì—°ê²° ì•ˆ ë¨

```bash
# í¬íŠ¸ í™•ì¸
lerobot-find-port

# ê¶Œí•œ í™•ì¸ (Linux)
sudo chmod 666 /dev/ttyACM0

# ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í™•ì¸
lerobot-calibrate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_so101
```

---

## ê³ ê¸‰ ì‚¬ìš©

### PolicyServerì— ë°ì´í„° ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€

ì„œë²„ì—ì„œ observation + actionì„ ì €ì¥í•˜ë ¤ë©´ `policy_server.py` ìˆ˜ì •:

```python
# policy_server.py ìˆ˜ì •

class PolicyServer(services_pb2_grpc.AsyncInferenceServicer):
    def __init__(self, config: PolicyServerConfig):
        # ... ê¸°ì¡´ ì½”ë“œ ...

        # ë°ì´í„°ì…‹ ì €ì¥ ì¶”ê°€
        if config.record_dataset:
            from lerobot.datasets.lerobot_dataset import LeRobotDataset
            self.dataset = LeRobotDataset.create(
                repo_id=config.dataset_repo_id,
                fps=config.fps,
                robot_type="so101_follower",
                features=...,
                use_videos=True,
            )
        else:
            self.dataset = None

    def _predict_action_chunk(self, obs):
        action_chunk = self.policy.predict_action_chunk(obs)

        # ë°ì´í„° ì €ì¥
        if self.dataset is not None:
            frame = {**obs, **action_chunk}
            self.dataset.add_frame(frame)

        return action_chunk
```

---

## ì°¸ê³  ìë£Œ

- **ì •ì±… íŒŒì¼**: `src/lerobot/policies/heuristic/`
- **ì˜ˆì œ**: `examples/heuristic_policy_example.py`
- **README**: `src/lerobot/policies/heuristic/README.md`
- **ì„¤ì •**: `HeuristicConfig` in `configuration_heuristic.py`

---

## ë‹¤ìŒ ë‹¨ê³„

1. âœ… ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸
2. âœ… ì»¤ìŠ¤í…€ íœ´ë¦¬ìŠ¤í‹± êµ¬í˜„
3. âœ… PolicyServerë¡œ ì›ê²© ì œì–´
4. âœ… ë°ì´í„° ìˆ˜ì§‘
5. âœ… í•™ìŠµëœ ì •ì±…ê³¼ ë¹„êµ

**Good luck!** ğŸš€
